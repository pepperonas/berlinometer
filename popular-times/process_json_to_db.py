#!/usr/bin/env python3
"""
Script to process JSON files generated by the scraper and insert them into the database.
This script is designed to be run after the scraper completes.
"""

import json
import os
import sys
import mysql.connector
from mysql.connector import pooling
from datetime import datetime
from dotenv import load_dotenv
import logging

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# MySQL Connection Pool
def create_db_pool():
    """Create MySQL connection pool"""
    try:
        db_config = {
            'host': os.getenv('MYSQL_HOST', 'localhost'),
            'user': os.getenv('MYSQL_USER', 'root'),
            'password': os.getenv('MYSQL_PASSWORD', ''),
            'database': os.getenv('MYSQL_DATABASE', 'popular_times_db'),
            'port': int(os.getenv('MYSQL_PORT', '3306'))
        }
        
        # Create connection pool
        db_pool = pooling.MySQLConnectionPool(
            pool_name="json_processor_pool",
            pool_size=3,
            **db_config
        )
        logger.info("âœ… MySQL connection pool created successfully")
        return db_pool
    except Exception as e:
        logger.error(f"âŒ Failed to create MySQL connection pool: {e}")
        return None

def save_to_database(result, db_pool):
    """
    Save a single scraping result to the MySQL database
    """
    if not db_pool:
        logger.error("No database connection available")
        return False
    
    try:
        conn = db_pool.get_connection()
        cursor = conn.cursor()
        
        # Extract data
        location_name = result.get('location_name', 'Unknown')
        live_occupancy = result.get('live_occupancy', '')
        is_live_data = result.get('is_live_data', False)
        
        # Extract occupancy percentages
        occupancy_percent = None
        usual_percent = None
        
        if live_occupancy:
            import re
            # Try to extract current percentage
            current_match = re.search(r'derzeit\s+zu\s+(\d+)\s*%\s*ausgelastet', live_occupancy, re.IGNORECASE)
            if current_match:
                occupancy_percent = int(current_match.group(1))
            else:
                # Fallback: any percentage
                percent_match = re.search(r'(\d+)\s*%', live_occupancy)
                if percent_match:
                    occupancy_percent = int(percent_match.group(1))
            
            # Try to extract usual percentage
            usual_match = re.search(r'normal\s+sind\s+(\d+)\s*%', live_occupancy, re.IGNORECASE)
            if usual_match:
                usual_percent = int(usual_match.group(1))
        
        # Stored Procedure call with correct 7 parameters
        cursor.callproc('insert_occupancy_data', [
            result.get('url', ''),                    # p_url
            location_name,                            # p_name
            result.get('address'),                    # p_address
            occupancy_percent,                        # p_occupancy_percent
            usual_percent,                            # p_usual_percent
            is_live_data,                             # p_is_live_data
            live_occupancy                            # p_raw_text
        ])
        
        conn.commit()
        cursor.close()
        conn.close()
        
        logger.info(f"âœ… Saved to database: {location_name}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Database error for {result.get('location_name', 'Unknown')}: {e}")
        if conn:
            conn.rollback()
            conn.close()
        return False

def process_json_file(filepath, db_pool):
    """Process a single JSON file and insert data into database"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Check if it's the new webapp format or old format
        if 'results' in data:
            # Webapp format
            results = data.get('results', [])
            logger.info(f"ğŸ“„ Processing webapp format JSON with {len(results)} results")
        elif 'locations' in data:
            # Old format
            results = data.get('locations', [])
            logger.info(f"ğŸ“„ Processing old format JSON with {len(results)} locations")
        else:
            logger.warning(f"âš ï¸ Unknown JSON format in {filepath}")
            return 0
        
        success_count = 0
        for result in results:
            if save_to_database(result, db_pool):
                success_count += 1
        
        logger.info(f"âœ… Processed {success_count}/{len(results)} entries from {os.path.basename(filepath)}")
        return success_count
        
    except Exception as e:
        logger.error(f"âŒ Error processing {filepath}: {e}")
        return 0

def find_latest_json_file():
    """Find the most recent JSON file in the scraper output directories"""
    # Check multiple possible locations
    possible_dirs = [
        '/var/www/html/popular-times/maps-playwrite-scraper',
        '/var/www/html/popular-times/popular-times-scrapings',
        os.path.join(os.path.dirname(os.path.abspath(__file__)), 'maps-playwrite-scraper'),
        os.path.join(os.path.dirname(os.path.abspath(__file__)), 'popular-times-scrapings')
    ]
    
    latest_file = None
    latest_time = 0
    
    for directory in possible_dirs:
        if not os.path.exists(directory):
            continue
            
        logger.info(f"ğŸ” Checking directory: {directory}")
        
        for filename in os.listdir(directory):
            if filename.endswith('.json') and ('occupancy_data' in filename or 'scraping_' in filename):
                filepath = os.path.join(directory, filename)
                file_time = os.path.getmtime(filepath)
                
                if file_time > latest_time:
                    latest_time = file_time
                    latest_file = filepath
    
    return latest_file

def main():
    """Main function"""
    logger.info("ğŸš€ Starting JSON to Database processor")
    
    # Create database connection pool
    db_pool = create_db_pool()
    if not db_pool:
        logger.error("Failed to create database connection. Exiting.")
        sys.exit(1)
    
    # Check if a specific file was provided as argument
    if len(sys.argv) > 1:
        json_file = sys.argv[1]
        if os.path.exists(json_file):
            logger.info(f"ğŸ“ Processing specified file: {json_file}")
            process_json_file(json_file, db_pool)
        else:
            logger.error(f"âŒ File not found: {json_file}")
            sys.exit(1)
    else:
        # Find and process the latest JSON file
        latest_file = find_latest_json_file()
        if latest_file:
            logger.info(f"ğŸ“ Found latest file: {latest_file}")
            logger.info(f"ğŸ“… File modified: {datetime.fromtimestamp(os.path.getmtime(latest_file))}")
            process_json_file(latest_file, db_pool)
        else:
            logger.error("âŒ No JSON files found to process")
            sys.exit(1)
    
    logger.info("âœ… Processing complete")

if __name__ == "__main__":
    main()